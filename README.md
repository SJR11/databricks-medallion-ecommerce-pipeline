# Eâ€‘commerce Data Pipeline on Databricks

Welcome to the **Eâ€‘commerce Data Pipeline on Databricks** repository. ğŸš€
This project implements an endâ€‘toâ€‘end data engineering workflow on Databricks, using the Medallion Architecture (Bronze â†’ Silver â†’ Gold) to build an analyticsâ€‘ready dataset for an online retail business.

---
## ğŸ—ï¸ Data Architecture

The data architecture for this project follows Medallion Architecture **Bronze**, **Silver**, and **Gold** layers:
<img width="17234" height="7528" alt="databricks_architecture" src="https://github.com/user-attachments/assets/6c6226bc-ff8e-484f-a6f4-42df4e5c5023" />
<img width="18554" height="7390" alt="legacy_architecture" src="https://github.com/user-attachments/assets/71176b23-5945-4c26-9c66-de59ae6fb43c" />


1. **Bronze Layer**: Ingests raw CSV files (orders, customers, products, brands, categories, dates) into Delta tables with minimal transformation, preserving the original data.
2. **Silver Layer**: Cleans, standardizes, and models data into dimension tables (customers, products, brands, categories, dates) with businessâ€‘friendly column names and data types.
3. **Gold Layer**: Builds fact tables for eâ€‘commerce orders and denormalized views that are optimized for reporting, Genie queries, and dashboarding.

---
## ğŸ“– Project Overview

This project demonstrates how to go from raw files to business insights using Databricks Free Edition.

### Key objectives:
1. Design and implement a Medallionâ€‘style lakehouse for an eâ€‘commerce use case.â€‹
2. Build incremental data processing notebooks for dimension and fact tables using Spark and Delta Lake.â€‹â€‹
3. Expose curated data for selfâ€‘service analytics through Genie and simple BI dashboards.

ğŸ¯ This repository is suitable to showcase skills in:
- Databricks & Spark data engineering
- Medallion architecture and Delta Lake
- ETL / ELT pipeline development 
- Data modeling for analytics (dimensions and facts) 
- SQLâ€‘based analysis and dashboarding  

---

## ğŸ› ï¸ Tech Stack & Tools

- **Platform:** Databricks Free Edition (Unity Catalog, Workflows)
- **Processing:** PySpark / Spark SQL, Delta Lake tables
- **Storage format:** Delta (Bronze, Silver, Gold tables)
- **Analytics:** Databricks SQL, Genie, Databricks dashboards

---

## ğŸš€ Project Requirements

Highâ€‘level business requirement: build a reliable analytics layer for an eâ€‘commerce companyâ€™s order data so stakeholders can analyze customers, products, and sales performance.

#### Main engineering requirements:

1. Ingest order data from CSV files into Databricks using the Bronze layer.â€‹â€‹
2. Clean and standardize data, resolving dataâ€‘quality issues (nulls, types, basic validation) in the Silver layer.â€‹
3. Model Goldâ€‘layer fact and dimension tables to support reporting on customers, products, brands, categories, and daily sales.â€‹â€‹
4. Provide a denormalized table or view for Genie and dashboard queries.

#### Analytics requirements:

1. Analyze customer behavior (order counts, revenue, repeat customers).â€‹â€‹
2. Evaluate product and brand performance (top sellers, revenue, discount impact).â€‹â€‹
3. Track sales trends over time using the date dimension (daily, monthly, seasonal views).

---

## ğŸ“‚ Repository Structure
```
databricks-ecommerce-medallion/
â”‚
â”œâ”€â”€ 0_data/                               # Raw input data files
â”‚   â””â”€â”€ order_items/
â”‚       â”œâ”€â”€ brands.csv
â”‚       â”œâ”€â”€ category.csv
â”‚       â”œâ”€â”€ customers.csv
â”‚       â”œâ”€â”€ date.csv
â”‚       â””â”€â”€ products.csv
â”‚
â”œâ”€â”€ 1_codes/                              # Databricks notebooks (exported as .ipynb)
â”‚   â”œâ”€â”€ 1_setup/
â”‚   â”‚   â””â”€â”€ setup_catalog.ipynb           # Catalog, schemas, and initial configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ 2_medallion_processing_dim/
â”‚   â”‚   â”œâ”€â”€ 1_dim_bronze.ipynb            # Load raw dimension data into Bronze tables
â”‚   â”‚   â”œâ”€â”€ 2_dim_silver.ipynb            # Clean and standardize dimension data (Silver)
â”‚   â”‚   â””â”€â”€ 3_dim_gold.ipynb              # Create dimension tables for analytics (Gold)
â”‚   â”‚
â”‚   â”œâ”€â”€ 3_medallion_processing_fact/
â”‚        â”œâ”€â”€ 1_fact_bronze.ipynb           # Load raw order/fact data into Bronze
â”‚        â”œâ”€â”€ 2_fact_silver.ipynb           # Transform and join for clean fact tables (Silver)
â”‚        â””â”€â”€ 3_fact_gold.ipynb             # Final fact tables and denormalized views (Gold)
â”‚   
â”‚â”€â”€ 2_genie_exploration/
â”‚       â””â”€â”€ questions.txt                 # Sample Genie prompts and business questions
â”‚
â”œâ”€â”€ 3_dashboard/
â”‚   â”œâ”€â”€ dashboard.png                     # Screenshot of Databricks dashboard
â”‚   â””â”€â”€ denormalise_table_query.sql       # SQL query used for the dashboard / Genie
â”‚
â”œâ”€â”€ LICENSE                               # Project license
â”œâ”€â”€ README.md                             # Project overview and usage instructions
â””â”€â”€ databricks_architecture.png
â””â”€â”€ legacy_architecture.png

```
---

## ğŸ›¡ï¸ License

This project is licensed under the [MIT License](LICENSE). You are free to use, modify, and share this project with proper attribution.

## ğŸŒŸ About Me

Hi! This project is maintained by **Er. Srijan Khadka**, an aspiring data engineer building endâ€‘toâ€‘end cloud data projects with Databricks, SQL, and modern data architecture.
